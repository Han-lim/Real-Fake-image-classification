{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "0601_EffNet04+XcpNet_soft.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Han-lim/Real-Fake-image-classification/blob/main/0601_EffNet04%2BXcpNet_soft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3nJon8jgnI"
      },
      "source": [
        "#### 현재까지의 best score 87.64 : \n",
        "EfficientNet7(Ensemble(Soft voting) + Xception Net + ... 에 대한 Hard voting)\n",
        "\n",
        "------\n",
        "##1. Optimizer, LR Scheduler 시행착오:  \n",
        " - Optimizer: adam, radam\n",
        " - stepLR, cosineAnnealingLR, custom cosine annealing warmup restarts(SGDR)\n",
        "\n",
        "<strong> * 사용한 조합 : radam + cosineAnnealingLR </strong>\n",
        "<br></br>\n",
        "\n",
        "----\n",
        "##2. Augmentation 시행착오:\n",
        "- HorizontalFlip, Resize(128, 128)  \n",
        "- cutout, GridDistortion \n",
        "\n",
        "<strong> * 다양한 augmentation을 시도해본 후, resize(128, 128)만 사용하는 것으로 결정함. </strong>\n",
        "<br></br>\n",
        "\n",
        "---\n",
        "## 3. Model\n",
        "- Efficient Net\n",
        "- Xception net \n",
        "\n",
        "<strong>* 위의 두 모델을 중점으로 앙상블을 적용함 </strong>\n",
        "<br></br>\n",
        "\n",
        "-----\n",
        "##4. dropout :\n",
        "- efficient net(dropout:0.2), xception net(dropout: 0.5)\n",
        "<strong> *  drop out은 모델별로 효과가 상이함 </strong>\n",
        "<br></br>\n",
        "\n",
        "-----\n",
        "\n",
        "##5. 앙상블 방법 :\n",
        "- soft voting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQsl9cEJv3EN"
      },
      "source": [
        "# 1. library 설치 및 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKmbI0Gy_7xW",
        "outputId": "d22c7161-04a9-4e8e-b181-b02ccc45e01d"
      },
      "source": [
        "from typing import Tuple, List, Sequence, Callable\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "!pip install -U git+https://github.com/albu/albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-1of1kren\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-1of1kren\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.0) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.15.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-1.0.0-cp37-none-any.whl size=98140 sha256=258bee4dc861682bf2db3880d9070b9e9b499dec456b7ae80d08fb1529d77eab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pyjcc1p8/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV_AvcTi9lHy",
        "outputId": "ab231359-d0a7-4a0c-fa1e-c723bca82c23"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QON1y-D2-YJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d251a3-499c-46a2-8fe1-6435757043aa"
      },
      "source": [
        "# Efficient Net 다운로드\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=45398777baa77049b1e931cf59ff8d7f554f1d5da23aafde8170b214494d0465\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzjX3KmmuWAs",
        "outputId": "4ec48193-76c2-4143-ceba-8d4e5caf01c3"
      },
      "source": [
        "# Xception Net 다운로드\n",
        "!pip install timm\n",
        "import timm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/08/1ccaf8d516935666b7fa5f6aaddf157c66208ea0c93bb847ae09f166354f/timm-0.4.9-py3-none-any.whl (346kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 20.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 16.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xeq5ohou7Lg",
        "outputId": "110310fb-e06f-488d-8065-e117d37ef64f"
      },
      "source": [
        "# optimizer 다운로드\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0f/bc49a0f714a1896b80f31db9ba82eebcb2bad9e0f5757184574f8ecfe2f1/torch_optimizer-0.1.0-py3-none-any.whl (72kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 25.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 30kB 27.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 61kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.8.1+cu101)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch_optimizer) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch_optimizer) (1.19.5)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb8Fa29ADg5E",
        "outputId": "b6aab235-ac94-4ead-9882-19307707df6d"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG0JBFolDiD6"
      },
      "source": [
        "device = \"cuda:0\"\n",
        "dtype = torch.float\n",
        "ltype = torch.long # entropy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU2QCdNfvyZX"
      },
      "source": [
        "# 2. 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "pxWSa54QD1AI",
        "outputId": "ffdcc31a-0d01-49f4-9a9b-088350e37d2c"
      },
      "source": [
        "# fake 1, real 0 로 사용\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/통기계플젝/face_image/face_images.csv')\n",
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>real</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./face_image/fake/JFH50GFJUL.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./face_image/fake/0VPS5TI60G.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./face_image/real/61911.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./face_image/fake/APADHGXN31.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./face_image/fake/SJO2UL69C2.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path  real  fake\n",
              "0  ./face_image/fake/JFH50GFJUL.jpg     0     1\n",
              "1  ./face_image/fake/0VPS5TI60G.jpg     0     1\n",
              "2       ./face_image/real/61911.jpg     1     0\n",
              "3  ./face_image/fake/APADHGXN31.jpg     0     1\n",
              "4  ./face_image/fake/SJO2UL69C2.jpg     0     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPmezkuOD2-3",
        "outputId": "0a1ae3e4-9a15-4e85-db43-b3157306a3c8"
      },
      "source": [
        "print(train_df.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdoK1VzVNaNM"
      },
      "source": [
        "# 경로 수정\n",
        "tmp = train_df['path'].str[2:]\n",
        "train_df['path'] = '/content/drive/MyDrive/Colab Notebooks/통기계플젝/'+tmp"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "lxlIc_E9PZAB",
        "outputId": "870ce2ea-78e4-431b-9aee-7743609d07af"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>real</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/통기계...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/통기계...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/통기계...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/통기계...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/통기계...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  real  fake\n",
              "0  /content/drive/MyDrive/Colab Notebooks/통기계...     0     1\n",
              "1  /content/drive/MyDrive/Colab Notebooks/통기계...     0     1\n",
              "2  /content/drive/MyDrive/Colab Notebooks/통기계...     1     0\n",
              "3  /content/drive/MyDrive/Colab Notebooks/통기계...     0     1\n",
              "4  /content/drive/MyDrive/Colab Notebooks/통기계...     0     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yjseivy1GwB"
      },
      "source": [
        "# 재현을 위한 seed 고정\n",
        "import random\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS_BBe_XvmsU"
      },
      "source": [
        "# 3. 데이터 셋 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfmZq_mwESI7"
      },
      "source": [
        "class FaceDataset(Dataset):\n",
        "  def __init__(self, image_label, transforms) :\n",
        "    self.df = image_label\n",
        "    self.transforms = transforms \n",
        "        \n",
        "  def __len__(self) -> int:\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "    assert index <= len(self), 'index range error' \n",
        "      \n",
        "    image_dir = self.df.iloc[index, ]['path']  \n",
        "    image_id = self.df.iloc[index, ]['fake'].astype(np.int64)\n",
        "    \n",
        "    image =  cv2.imread(image_dir, cv2.COLOR_BGR2RGB)\n",
        "    target = torch.as_tensor(image_id, dtype=torch.long)\n",
        "\n",
        "    if self.transforms is not None :\n",
        "      image = self.transforms(image=image)['image']\n",
        "\n",
        "    image = image/255.0  # numpy array를 0~1사이의 실수를 갖도록 변환\n",
        "    \n",
        "    return image, target"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtRIYJ6KeV91"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "  def __init__(self, image, transforms) :\n",
        "    self.image = image\n",
        "    self.transforms = transforms\n",
        "        \n",
        "  def __len__(self) -> int:\n",
        "    return len(self.image)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "    assert index <= len(self), 'index range error' \n",
        "    \n",
        "    image_name = self.image[index]\n",
        "    image_dir = '/content/drive/MyDrive/Colab Notebooks/통기계플젝/face_image/test/' + image_name\n",
        "\n",
        "    image =  cv2.imread(image_dir, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    if self.transforms is not None :\n",
        "      image = self.transforms(image=image)['image']\n",
        "    image = image/255.0\n",
        "\n",
        "    return image_name, image"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIFqDN-Ncbxa"
      },
      "source": [
        "# Image Augmentation\n",
        "transforms_tr = A.Compose([\n",
        "    A.Resize(128, 128),\n",
        "    ToTensorV2(), ])\n",
        "\n",
        "transforms_val = A.Compose([\n",
        "   A.Resize(128, 128),\n",
        "    ToTensorV2(), ])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYkAde0IviS2"
      },
      "source": [
        "# 4. 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiWgPhBIJWsr",
        "outputId": "3ff5baf0-f6fc-4e1f-9b9b-52b7ee0bb35d"
      },
      "source": [
        "# efficient net 정의\n",
        "class efficientnet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(efficientnet, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(3, 3, 3, stride=1)\n",
        "        self.effnet = EfficientNet.from_name('efficientnet-b4', dropout_rate=0.2) # 구조만 사용\n",
        "        self.FC = nn.Linear(1000, 2)\n",
        "\n",
        "        nn.init.xavier_normal_(self.FC.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.silu(self.conv2d(x))\n",
        "\n",
        "        # effnet을 추가\n",
        "        x = F.silu(self.effnet(x))\n",
        "\n",
        "        # 마지막 출력에 nn.Linear를 추가\n",
        "        x = self.FC(x)\n",
        "        return x\n",
        "\n",
        "model = efficientnet()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "efficientnet(\n",
              "  (conv2d): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (effnet): EfficientNet(\n",
              "    (_conv_stem): Conv2dStaticSamePadding(\n",
              "      3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "      (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "    )\n",
              "    (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "    (_blocks): ModuleList(\n",
              "      (0): MBConvBlock(\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (1): MBConvBlock(\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (2): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (3): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (4): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (5): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (6): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (7): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (8): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (9): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (10): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (11): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (12): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (13): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (14): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (15): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (16): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (17): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (18): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (19): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (20): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (21): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (22): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (23): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (24): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (25): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (26): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (27): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (28): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (29): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (30): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "      (31): MBConvBlock(\n",
              "        (_expand_conv): Conv2dStaticSamePadding(\n",
              "          448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "          2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
              "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "        )\n",
              "        (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_se_reduce): Conv2dStaticSamePadding(\n",
              "          2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_se_expand): Conv2dStaticSamePadding(\n",
              "          112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_project_conv): Conv2dStaticSamePadding(\n",
              "          2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "          (static_padding): Identity()\n",
              "        )\n",
              "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "        (_swish): MemoryEfficientSwish()\n",
              "      )\n",
              "    )\n",
              "    (_conv_head): Conv2dStaticSamePadding(\n",
              "      448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "      (static_padding): Identity()\n",
              "    )\n",
              "    (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "    (_dropout): Dropout(p=0.2, inplace=False)\n",
              "    (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
              "    (_swish): MemoryEfficientSwish()\n",
              "  )\n",
              "  (FC): Linear(in_features=1000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dTT4YvUlJ1A"
      },
      "source": [
        "# xception net 정의\n",
        "class xceptionnet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xceptionnet, self).__init__()\n",
        "        self.xcpnet = timm.create_model('xception', pretrained=False) # 구조만 사용\n",
        "        self.FC = nn.Linear(1000, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.xcpnet(x)\n",
        "        x = self.FC(x)\n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__kNyQbN3_MD",
        "outputId": "8e32507e-5979-45ba-a0ac-f5387ed69472"
      },
      "source": [
        "xceptionnet()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "xceptionnet(\n",
              "  (xcpnet): Xception(\n",
              "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act1): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act2): ReLU(inplace=True)\n",
              "    (block1): Block(\n",
              "      (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (rep): Sequential(\n",
              "        (0): SeparableConv2d(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): SeparableConv2d(\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (block2): Block(\n",
              "      (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "          (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "          (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (block3): Block(\n",
              "      (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "          (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (block4): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block5): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block6): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block7): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block8): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block9): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block10): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block11): Block(\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (block12): Block(\n",
              "      (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (rep): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): SeparableConv2d(\n",
              "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "          (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (conv3): SeparableConv2d(\n",
              "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act3): ReLU(inplace=True)\n",
              "    (conv4): SeparableConv2d(\n",
              "      (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
              "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act4): ReLU(inplace=True)\n",
              "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n",
              "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  )\n",
              "  (FC): Linear(in_features=1000, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUe8dvP0vcIX"
      },
      "source": [
        "# Efficient Net train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z03D-NYsFpbV",
        "outputId": "cd97d04e-dfa3-46c9-c498-a49b72412303"
      },
      "source": [
        "###########################################################################\n",
        "# - n_splits = 10: train/valid set split을 9대 1로 하려고 설정            #\n",
        "# - CUDA Memory 문제로 3fold 까지만 진행                                  #\n",
        "# - train 이후에 런타임 초기화해서 저장한 best model들 다시 호출 후 test  #\n",
        "#                                                                         #\n",
        "###########################################################################\n",
        "num_epochs = 30\n",
        "EARLY_STOPPING_EPOCH = 6\n",
        "SEED = 42\n",
        "\n",
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
        "\n",
        "# train_df에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(train_df),1): # enumerate( , 1) 1부터 index 시작\n",
        "  print('[fold: %d]' % fold_index)\n",
        "\n",
        "  # cuda cache 초기화\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  #train fold, validation fold 분할\n",
        "  train = train_df.iloc[trn_idx]\n",
        "  valid  = train_df.iloc[val_idx]\n",
        "\n",
        "  tr_dataset = FaceDataset(image_label=train, transforms=transforms_tr)\n",
        "  val_dataset = FaceDataset(image_label=valid, transforms=transforms_val)\n",
        "\n",
        "  train_loader = DataLoader(tr_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
        "  valid_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
        "\n",
        "  # 모델 선언\n",
        "  model = efficientnet()\n",
        "  model.to(device)\n",
        "\n",
        "  # optimizer : RAdam / lr_scheduler : CosineAnnealing\n",
        "  optimizer = optim.RAdam(model.parameters(), lr=0.0015, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "  lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "\n",
        "  # train 시작\n",
        "  valid_early_stop = 0\n",
        "\n",
        "  valid_best_loss = float('inf')\n",
        "  since = time.time()\n",
        "\n",
        "  final_train_loss = []\n",
        "  final_train_acc = []\n",
        "  final_valid_loss = []\n",
        "  final_valid_acc = []\n",
        "\n",
        "  for e in range(num_epochs) :\n",
        "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
        "    train_loss_list = []\n",
        "    train_acc_list = []\n",
        "\n",
        "    # train\n",
        "    for i, (images, targets) in enumerate(train_loader) : \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      images = images.to(device, dtype)\n",
        "      targets = targets.to(device, ltype)\n",
        "      model.train()\n",
        "    \n",
        "      scores = model(images)\n",
        "      _, preds = scores.max(dim=1)\n",
        "\n",
        "      loss = F.cross_entropy(scores, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      correct = sum(targets == preds).cpu()\n",
        "      acc=(correct/64 * 100)\n",
        "\n",
        "      train_loss_list.append(loss)\n",
        "      train_acc_list.append(acc)\n",
        "\n",
        "      if i % 20 == 0 :\n",
        "        print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
        "\n",
        "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
        "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
        "\n",
        "    final_train_loss.append(train_mean_loss)\n",
        "    final_train_acc.append(train_mean_acc)\n",
        "    \n",
        "    epoch_time = time.time() - since\n",
        "    since = time.time()\n",
        "\n",
        "    print('')\n",
        "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
        "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
        "\n",
        "    # validation \n",
        "    valid_loss_list = []\n",
        "    valid_acc_list = []\n",
        "    for i, (images, targets) in enumerate(valid_loader) : \n",
        "      optimizer.zero_grad()\n",
        "      images = images.to(device=device, dtype=dtype)\n",
        "      targets = targets.to(device=device, dtype=ltype)\n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        scores = model(images)\n",
        "        loss = F.cross_entropy(scores, targets)\n",
        "        _, preds = scores.max(dim=1)\n",
        "      \n",
        "      correct = sum(targets == preds).cpu()\n",
        "      acc=(correct/64 * 100)\n",
        "\n",
        "      valid_loss_list.append(loss)\n",
        "      valid_acc_list.append(acc)\n",
        "  \n",
        "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
        "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
        "\n",
        "    final_valid_loss.append(val_mean_loss)\n",
        "    final_valid_acc.append(val_mean_acc)\n",
        "\n",
        "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
        "    print('')\n",
        "\n",
        "    if val_mean_loss < valid_best_loss:\n",
        "      valid_best_loss = val_mean_loss\n",
        "      valid_early_stop = 0\n",
        "      # new best model save (valid 기준)\n",
        "      best_model = model\n",
        "      path = '/content/drive/MyDrive/Colab Notebooks/통기계플젝/model/efficient/'\n",
        "      torch.save({'model_state_dict' : best_model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict()}, f'{path}{fold_index}-fold_best-model.pth')\n",
        "\n",
        "    else:\n",
        "      # early stopping    \n",
        "      valid_early_stop += 1\n",
        "      if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
        "        print(\"EARLY STOPPING!!\")\n",
        "        break\n",
        "\n",
        "    lr_sched.step()\n",
        "  best_models.append(best_model)\n",
        "  if fold_index == 3:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fold: 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ====================== epoch 1 ======================\n",
            "Iteration   0 | Train Loss  0.7075 | Classifier Accuracy 50.00\n",
            "Iteration  20 | Train Loss  0.7088 | Classifier Accuracy 57.81\n",
            "Iteration  40 | Train Loss  0.7213 | Classifier Accuracy 51.56\n",
            "Iteration  60 | Train Loss  0.6994 | Classifier Accuracy 56.25\n",
            "Iteration  80 | Train Loss  0.6788 | Classifier Accuracy 59.38\n",
            "Iteration 100 | Train Loss  0.7087 | Classifier Accuracy 51.56\n",
            "Iteration 120 | Train Loss  0.7461 | Classifier Accuracy 50.00\n",
            "Iteration 140 | Train Loss  0.7721 | Classifier Accuracy 46.88\n",
            "Iteration 160 | Train Loss  0.6641 | Classifier Accuracy 64.06\n",
            "Iteration 180 | Train Loss  0.7192 | Classifier Accuracy 59.38\n",
            "Iteration 200 | Train Loss  0.6302 | Classifier Accuracy 62.50\n",
            "Iteration 220 | Train Loss  0.6038 | Classifier Accuracy 70.31\n",
            "Iteration 240 | Train Loss  0.6099 | Classifier Accuracy 75.00\n",
            "Iteration 260 | Train Loss  0.5982 | Classifier Accuracy 70.31\n",
            "Iteration 280 | Train Loss  0.6298 | Classifier Accuracy 64.06\n",
            "\n",
            "[Summary] Elapsed time : 7 m 59 s\n",
            "Train Loss Mean 0.6762 | Accuracy 58.34 \n",
            "Valid Loss Mean 0.6945 | Accuracy 48.05 \n",
            "\n",
            " ====================== epoch 2 ======================\n",
            "Iteration   0 | Train Loss  0.6204 | Classifier Accuracy 71.88\n",
            "Iteration  20 | Train Loss  0.5869 | Classifier Accuracy 65.62\n",
            "Iteration  40 | Train Loss  0.4837 | Classifier Accuracy 75.00\n",
            "Iteration  60 | Train Loss  0.5809 | Classifier Accuracy 67.19\n",
            "Iteration  80 | Train Loss  0.4655 | Classifier Accuracy 79.69\n",
            "Iteration 100 | Train Loss  0.5289 | Classifier Accuracy 71.88\n",
            "Iteration 120 | Train Loss  0.5004 | Classifier Accuracy 75.00\n",
            "Iteration 140 | Train Loss  0.4943 | Classifier Accuracy 85.94\n",
            "Iteration 160 | Train Loss  0.4757 | Classifier Accuracy 81.25\n",
            "Iteration 180 | Train Loss  0.4543 | Classifier Accuracy 81.25\n",
            "Iteration 200 | Train Loss  0.4888 | Classifier Accuracy 75.00\n",
            "Iteration 220 | Train Loss  0.5281 | Classifier Accuracy 73.44\n",
            "Iteration 240 | Train Loss  0.6045 | Classifier Accuracy 71.88\n",
            "Iteration 260 | Train Loss  0.4350 | Classifier Accuracy 78.12\n",
            "Iteration 280 | Train Loss  0.4654 | Classifier Accuracy 76.56\n",
            "\n",
            "[Summary] Elapsed time : 3 m 6 s\n",
            "Train Loss Mean 0.5195 | Accuracy 74.48 \n",
            "Valid Loss Mean 0.6304 | Accuracy 65.48 \n",
            "\n",
            " ====================== epoch 3 ======================\n",
            "Iteration   0 | Train Loss  0.4253 | Classifier Accuracy 79.69\n",
            "Iteration  20 | Train Loss  0.4286 | Classifier Accuracy 84.38\n",
            "Iteration  40 | Train Loss  0.4570 | Classifier Accuracy 79.69\n",
            "Iteration  60 | Train Loss  0.4254 | Classifier Accuracy 79.69\n",
            "Iteration  80 | Train Loss  0.5053 | Classifier Accuracy 73.44\n",
            "Iteration 100 | Train Loss  0.5456 | Classifier Accuracy 73.44\n",
            "Iteration 120 | Train Loss  0.4696 | Classifier Accuracy 79.69\n",
            "Iteration 140 | Train Loss  0.3218 | Classifier Accuracy 85.94\n",
            "Iteration 160 | Train Loss  0.4220 | Classifier Accuracy 82.81\n",
            "Iteration 180 | Train Loss  0.4122 | Classifier Accuracy 84.38\n",
            "Iteration 200 | Train Loss  0.3849 | Classifier Accuracy 84.38\n",
            "Iteration 220 | Train Loss  0.4281 | Classifier Accuracy 79.69\n",
            "Iteration 240 | Train Loss  0.3636 | Classifier Accuracy 84.38\n",
            "Iteration 260 | Train Loss  0.2954 | Classifier Accuracy 89.06\n",
            "Iteration 280 | Train Loss  0.3118 | Classifier Accuracy 87.50\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.4257 | Accuracy 80.54 \n",
            "Valid Loss Mean 0.3908 | Accuracy 80.86 \n",
            "\n",
            " ====================== epoch 4 ======================\n",
            "Iteration   0 | Train Loss  0.3457 | Classifier Accuracy 87.50\n",
            "Iteration  20 | Train Loss  0.3027 | Classifier Accuracy 89.06\n",
            "Iteration  40 | Train Loss  0.4005 | Classifier Accuracy 81.25\n",
            "Iteration  60 | Train Loss  0.2420 | Classifier Accuracy 89.06\n",
            "Iteration  80 | Train Loss  0.1465 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.3521 | Classifier Accuracy 84.38\n",
            "Iteration 120 | Train Loss  0.3151 | Classifier Accuracy 89.06\n",
            "Iteration 140 | Train Loss  0.2702 | Classifier Accuracy 87.50\n",
            "Iteration 160 | Train Loss  0.4662 | Classifier Accuracy 78.12\n",
            "Iteration 180 | Train Loss  0.4502 | Classifier Accuracy 78.12\n",
            "Iteration 200 | Train Loss  0.3256 | Classifier Accuracy 89.06\n",
            "Iteration 220 | Train Loss  0.3499 | Classifier Accuracy 78.12\n",
            "Iteration 240 | Train Loss  0.2735 | Classifier Accuracy 89.06\n",
            "Iteration 260 | Train Loss  0.3637 | Classifier Accuracy 84.38\n",
            "Iteration 280 | Train Loss  0.2773 | Classifier Accuracy 90.62\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.3487 | Accuracy 84.70 \n",
            "Valid Loss Mean 0.3545 | Accuracy 82.67 \n",
            "\n",
            " ====================== epoch 5 ======================\n",
            "Iteration   0 | Train Loss  0.2246 | Classifier Accuracy 92.19\n",
            "Iteration  20 | Train Loss  0.2192 | Classifier Accuracy 93.75\n",
            "Iteration  40 | Train Loss  0.2048 | Classifier Accuracy 87.50\n",
            "Iteration  60 | Train Loss  0.2900 | Classifier Accuracy 85.94\n",
            "Iteration  80 | Train Loss  0.2003 | Classifier Accuracy 90.62\n",
            "Iteration 100 | Train Loss  0.3397 | Classifier Accuracy 87.50\n",
            "Iteration 120 | Train Loss  0.3342 | Classifier Accuracy 89.06\n",
            "Iteration 140 | Train Loss  0.1536 | Classifier Accuracy 93.75\n",
            "Iteration 160 | Train Loss  0.2929 | Classifier Accuracy 89.06\n",
            "Iteration 180 | Train Loss  0.3528 | Classifier Accuracy 89.06\n",
            "Iteration 200 | Train Loss  0.2104 | Classifier Accuracy 90.62\n",
            "Iteration 220 | Train Loss  0.2852 | Classifier Accuracy 90.62\n",
            "Iteration 240 | Train Loss  0.1935 | Classifier Accuracy 90.62\n",
            "Iteration 260 | Train Loss  0.2982 | Classifier Accuracy 84.38\n",
            "Iteration 280 | Train Loss  0.2722 | Classifier Accuracy 85.94\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.2591 | Accuracy 89.20 \n",
            "Valid Loss Mean 0.3312 | Accuracy 83.40 \n",
            "\n",
            " ====================== epoch 6 ======================\n",
            "Iteration   0 | Train Loss  0.1828 | Classifier Accuracy 93.75\n",
            "Iteration  20 | Train Loss  0.1579 | Classifier Accuracy 92.19\n",
            "Iteration  40 | Train Loss  0.1580 | Classifier Accuracy 93.75\n",
            "Iteration  60 | Train Loss  0.1558 | Classifier Accuracy 93.75\n",
            "Iteration  80 | Train Loss  0.0983 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.1468 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.0656 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0662 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.2605 | Classifier Accuracy 90.62\n",
            "Iteration 180 | Train Loss  0.1043 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.3753 | Classifier Accuracy 84.38\n",
            "Iteration 220 | Train Loss  0.1174 | Classifier Accuracy 95.31\n",
            "Iteration 240 | Train Loss  0.2951 | Classifier Accuracy 89.06\n",
            "Iteration 260 | Train Loss  0.1318 | Classifier Accuracy 93.75\n",
            "Iteration 280 | Train Loss  0.0985 | Classifier Accuracy 96.88\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.1867 | Accuracy 92.38 \n",
            "Valid Loss Mean 0.3417 | Accuracy 84.81 \n",
            "\n",
            " ====================== epoch 7 ======================\n",
            "Iteration   0 | Train Loss  0.1233 | Classifier Accuracy 96.88\n",
            "Iteration  20 | Train Loss  0.1135 | Classifier Accuracy 95.31\n",
            "Iteration  40 | Train Loss  0.1768 | Classifier Accuracy 90.62\n",
            "Iteration  60 | Train Loss  0.0500 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.1833 | Classifier Accuracy 95.31\n",
            "Iteration 100 | Train Loss  0.2431 | Classifier Accuracy 89.06\n",
            "Iteration 120 | Train Loss  0.1572 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.2111 | Classifier Accuracy 87.50\n",
            "Iteration 160 | Train Loss  0.0730 | Classifier Accuracy 98.44\n",
            "Iteration 180 | Train Loss  0.2212 | Classifier Accuracy 93.75\n",
            "Iteration 200 | Train Loss  0.1737 | Classifier Accuracy 89.06\n",
            "Iteration 220 | Train Loss  0.1431 | Classifier Accuracy 93.75\n",
            "Iteration 240 | Train Loss  0.2545 | Classifier Accuracy 89.06\n",
            "Iteration 260 | Train Loss  0.1109 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.3134 | Classifier Accuracy 85.94\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.1581 | Accuracy 93.67 \n",
            "Valid Loss Mean 0.3656 | Accuracy 85.45 \n",
            "\n",
            " ====================== epoch 8 ======================\n",
            "Iteration   0 | Train Loss  0.1204 | Classifier Accuracy 95.31\n",
            "Iteration  20 | Train Loss  0.2040 | Classifier Accuracy 93.75\n",
            "Iteration  40 | Train Loss  0.1466 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.1518 | Classifier Accuracy 92.19\n",
            "Iteration  80 | Train Loss  0.2822 | Classifier Accuracy 85.94\n",
            "Iteration 100 | Train Loss  0.0531 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.1554 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.0543 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.2164 | Classifier Accuracy 89.06\n",
            "Iteration 180 | Train Loss  0.2106 | Classifier Accuracy 89.06\n",
            "Iteration 200 | Train Loss  0.1809 | Classifier Accuracy 95.31\n",
            "Iteration 220 | Train Loss  0.1565 | Classifier Accuracy 92.19\n",
            "Iteration 240 | Train Loss  0.1888 | Classifier Accuracy 92.19\n",
            "Iteration 260 | Train Loss  0.1570 | Classifier Accuracy 93.75\n",
            "Iteration 280 | Train Loss  0.2225 | Classifier Accuracy 90.62\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.1523 | Accuracy 93.74 \n",
            "Valid Loss Mean 0.3635 | Accuracy 84.72 \n",
            "\n",
            " ====================== epoch 9 ======================\n",
            "Iteration   0 | Train Loss  0.2107 | Classifier Accuracy 90.62\n",
            "Iteration  20 | Train Loss  0.0362 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.1038 | Classifier Accuracy 95.31\n",
            "Iteration  60 | Train Loss  0.1316 | Classifier Accuracy 95.31\n",
            "Iteration  80 | Train Loss  0.0283 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0998 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.0923 | Classifier Accuracy 95.31\n",
            "Iteration 140 | Train Loss  0.1745 | Classifier Accuracy 92.19\n",
            "Iteration 160 | Train Loss  0.1076 | Classifier Accuracy 93.75\n",
            "Iteration 180 | Train Loss  0.1024 | Classifier Accuracy 95.31\n",
            "Iteration 200 | Train Loss  0.1697 | Classifier Accuracy 93.75\n",
            "Iteration 220 | Train Loss  0.0568 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0631 | Classifier Accuracy 96.88\n",
            "Iteration 260 | Train Loss  0.0612 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0820 | Classifier Accuracy 96.88\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0981 | Accuracy 96.15 \n",
            "Valid Loss Mean 0.3952 | Accuracy 83.89 \n",
            "\n",
            " ====================== epoch 10 ======================\n",
            "Iteration   0 | Train Loss  0.0809 | Classifier Accuracy 96.88\n",
            "Iteration  20 | Train Loss  0.0611 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.1141 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0524 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0499 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.1109 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.1297 | Classifier Accuracy 92.19\n",
            "Iteration 140 | Train Loss  0.0951 | Classifier Accuracy 96.88\n",
            "Iteration 160 | Train Loss  0.0825 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0623 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.1659 | Classifier Accuracy 92.19\n",
            "Iteration 220 | Train Loss  0.2005 | Classifier Accuracy 95.31\n",
            "Iteration 240 | Train Loss  0.2277 | Classifier Accuracy 85.94\n",
            "Iteration 260 | Train Loss  0.0656 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.0719 | Classifier Accuracy 95.31\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0915 | Accuracy 96.17 \n",
            "Valid Loss Mean 0.3015 | Accuracy 88.23 \n",
            "\n",
            " ====================== epoch 11 ======================\n",
            "Iteration   0 | Train Loss  0.0224 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0271 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.0727 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0134 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0179 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0100 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.1268 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.1651 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0703 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0240 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0681 | Classifier Accuracy 98.44\n",
            "Iteration 220 | Train Loss  0.0402 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0361 | Classifier Accuracy 98.44\n",
            "Iteration 260 | Train Loss  0.1323 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.0056 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.0611 | Accuracy 97.57 \n",
            "Valid Loss Mean 0.3726 | Accuracy 88.96 \n",
            "\n",
            " ====================== epoch 12 ======================\n",
            "Iteration   0 | Train Loss  0.0642 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.0111 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0312 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0093 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0686 | Classifier Accuracy 95.31\n",
            "Iteration 100 | Train Loss  0.0159 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0614 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0079 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0466 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0690 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.0140 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0997 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0277 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0586 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.0141 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0557 | Accuracy 97.66 \n",
            "Valid Loss Mean 0.4165 | Accuracy 86.08 \n",
            "\n",
            " ====================== epoch 13 ======================\n",
            "Iteration   0 | Train Loss  0.0133 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0776 | Classifier Accuracy 95.31\n",
            "Iteration  40 | Train Loss  0.1076 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.0839 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.3114 | Classifier Accuracy 93.75\n",
            "Iteration 100 | Train Loss  0.2238 | Classifier Accuracy 93.75\n",
            "Iteration 120 | Train Loss  0.2015 | Classifier Accuracy 96.88\n",
            "Iteration 140 | Train Loss  0.0905 | Classifier Accuracy 95.31\n",
            "Iteration 160 | Train Loss  0.1263 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0791 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.0669 | Classifier Accuracy 96.88\n",
            "Iteration 220 | Train Loss  0.0668 | Classifier Accuracy 96.88\n",
            "Iteration 240 | Train Loss  0.0812 | Classifier Accuracy 95.31\n",
            "Iteration 260 | Train Loss  0.0130 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0145 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0891 | Accuracy 96.49 \n",
            "Valid Loss Mean 0.2818 | Accuracy 88.87 \n",
            "\n",
            " ====================== epoch 14 ======================\n",
            "Iteration   0 | Train Loss  0.0203 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.0075 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0360 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0077 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0780 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0043 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0149 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0494 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0519 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0085 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0252 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0352 | Classifier Accuracy 96.88\n",
            "Iteration 240 | Train Loss  0.0636 | Classifier Accuracy 96.88\n",
            "Iteration 260 | Train Loss  0.0739 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.0346 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0410 | Accuracy 98.29 \n",
            "Valid Loss Mean 0.4988 | Accuracy 85.64 \n",
            "\n",
            " ====================== epoch 15 ======================\n",
            "Iteration   0 | Train Loss  0.0358 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0290 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.0860 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0365 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.1143 | Classifier Accuracy 93.75\n",
            "Iteration 100 | Train Loss  0.0878 | Classifier Accuracy 96.88\n",
            "Iteration 120 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0027 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0527 | Classifier Accuracy 98.44\n",
            "Iteration 180 | Train Loss  0.0413 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.0120 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0678 | Classifier Accuracy 96.88\n",
            "Iteration 240 | Train Loss  0.0127 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0367 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0335 | Accuracy 98.52 \n",
            "Valid Loss Mean 0.3216 | Accuracy 89.31 \n",
            "\n",
            " ====================== epoch 16 ======================\n",
            "Iteration   0 | Train Loss  0.0166 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0133 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0236 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0332 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0509 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.0087 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0127 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0161 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0049 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0008 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0044 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0062 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0271 | Classifier Accuracy 98.44\n",
            "Iteration 260 | Train Loss  0.0272 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0061 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 18 s\n",
            "Train Loss Mean 0.0258 | Accuracy 98.82 \n",
            "Valid Loss Mean 0.3713 | Accuracy 89.40 \n",
            "\n",
            " ====================== epoch 17 ======================\n",
            "Iteration   0 | Train Loss  0.0066 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0008 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0034 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0254 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0107 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0137 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0106 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0191 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0065 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0546 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.0031 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0098 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0083 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0651 | Classifier Accuracy 95.31\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0201 | Accuracy 99.02 \n",
            "Valid Loss Mean 0.3607 | Accuracy 89.60 \n",
            "\n",
            " ====================== epoch 18 ======================\n",
            "Iteration   0 | Train Loss  0.0153 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.0345 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0040 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0411 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0086 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0039 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0466 | Classifier Accuracy 96.88\n",
            "Iteration 140 | Train Loss  0.0101 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0032 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0165 | Classifier Accuracy 98.44\n",
            "Iteration 220 | Train Loss  0.0090 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0046 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0032 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0067 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 18 s\n",
            "Train Loss Mean 0.0166 | Accuracy 99.13 \n",
            "Valid Loss Mean 0.3295 | Accuracy 89.94 \n",
            "\n",
            " ====================== epoch 19 ======================\n",
            "Iteration   0 | Train Loss  0.0028 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0542 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0147 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0045 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0051 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0009 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0065 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0017 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0087 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0146 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.0087 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0215 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0010 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0090 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0057 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 18 s\n",
            "Train Loss Mean 0.0161 | Accuracy 99.10 \n",
            "Valid Loss Mean 0.3935 | Accuracy 90.04 \n",
            "\n",
            "EARLY STOPPING!!\n",
            "[fold: 2]\n",
            " ====================== epoch 1 ======================\n",
            "Iteration   0 | Train Loss  0.7386 | Classifier Accuracy 48.44\n",
            "Iteration  20 | Train Loss  0.8992 | Classifier Accuracy 51.56\n",
            "Iteration  40 | Train Loss  0.7528 | Classifier Accuracy 48.44\n",
            "Iteration  60 | Train Loss  0.8633 | Classifier Accuracy 43.75\n",
            "Iteration  80 | Train Loss  0.6851 | Classifier Accuracy 59.38\n",
            "Iteration 100 | Train Loss  0.6911 | Classifier Accuracy 50.00\n",
            "Iteration 120 | Train Loss  0.6911 | Classifier Accuracy 50.00\n",
            "Iteration 140 | Train Loss  0.7072 | Classifier Accuracy 50.00\n",
            "Iteration 160 | Train Loss  0.6774 | Classifier Accuracy 51.56\n",
            "Iteration 180 | Train Loss  0.6769 | Classifier Accuracy 56.25\n",
            "Iteration 200 | Train Loss  0.6651 | Classifier Accuracy 62.50\n",
            "Iteration 220 | Train Loss  0.7109 | Classifier Accuracy 60.94\n",
            "Iteration 240 | Train Loss  0.6727 | Classifier Accuracy 57.81\n",
            "Iteration 260 | Train Loss  0.5769 | Classifier Accuracy 68.75\n",
            "Iteration 280 | Train Loss  0.5961 | Classifier Accuracy 73.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 11 s\n",
            "Train Loss Mean 0.6904 | Accuracy 56.55 \n",
            "Valid Loss Mean 0.7063 | Accuracy 49.12 \n",
            "\n",
            " ====================== epoch 2 ======================\n",
            "Iteration   0 | Train Loss  0.6406 | Classifier Accuracy 57.81\n",
            "Iteration  20 | Train Loss  0.6458 | Classifier Accuracy 70.31\n",
            "Iteration  40 | Train Loss  0.5611 | Classifier Accuracy 81.25\n",
            "Iteration  60 | Train Loss  0.4825 | Classifier Accuracy 75.00\n",
            "Iteration  80 | Train Loss  0.6875 | Classifier Accuracy 64.06\n",
            "Iteration 100 | Train Loss  0.5079 | Classifier Accuracy 76.56\n",
            "Iteration 120 | Train Loss  0.4592 | Classifier Accuracy 71.88\n",
            "Iteration 140 | Train Loss  0.5942 | Classifier Accuracy 67.19\n",
            "Iteration 160 | Train Loss  0.4185 | Classifier Accuracy 81.25\n",
            "Iteration 180 | Train Loss  0.5893 | Classifier Accuracy 75.00\n",
            "Iteration 200 | Train Loss  0.3950 | Classifier Accuracy 81.25\n",
            "Iteration 220 | Train Loss  0.6174 | Classifier Accuracy 64.06\n",
            "Iteration 240 | Train Loss  0.5510 | Classifier Accuracy 67.19\n",
            "Iteration 260 | Train Loss  0.5482 | Classifier Accuracy 78.12\n",
            "Iteration 280 | Train Loss  0.4197 | Classifier Accuracy 82.81\n",
            "\n",
            "[Summary] Elapsed time : 2 m 25 s\n",
            "Train Loss Mean 0.5505 | Accuracy 72.09 \n",
            "Valid Loss Mean 0.6664 | Accuracy 58.59 \n",
            "\n",
            " ====================== epoch 3 ======================\n",
            "Iteration   0 | Train Loss  0.4602 | Classifier Accuracy 78.12\n",
            "Iteration  20 | Train Loss  0.4655 | Classifier Accuracy 78.12\n",
            "Iteration  40 | Train Loss  0.3637 | Classifier Accuracy 85.94\n",
            "Iteration  60 | Train Loss  0.5250 | Classifier Accuracy 76.56\n",
            "Iteration  80 | Train Loss  0.5616 | Classifier Accuracy 71.88\n",
            "Iteration 100 | Train Loss  0.4513 | Classifier Accuracy 79.69\n",
            "Iteration 120 | Train Loss  0.4465 | Classifier Accuracy 75.00\n",
            "Iteration 140 | Train Loss  0.4581 | Classifier Accuracy 78.12\n",
            "Iteration 160 | Train Loss  0.3750 | Classifier Accuracy 85.94\n",
            "Iteration 180 | Train Loss  0.5687 | Classifier Accuracy 76.56\n",
            "Iteration 200 | Train Loss  0.4368 | Classifier Accuracy 79.69\n",
            "Iteration 220 | Train Loss  0.4277 | Classifier Accuracy 82.81\n",
            "Iteration 240 | Train Loss  0.4205 | Classifier Accuracy 79.69\n",
            "Iteration 260 | Train Loss  0.3119 | Classifier Accuracy 84.38\n",
            "Iteration 280 | Train Loss  0.3597 | Classifier Accuracy 81.25\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.4473 | Accuracy 79.49 \n",
            "Valid Loss Mean 0.4310 | Accuracy 79.15 \n",
            "\n",
            " ====================== epoch 4 ======================\n",
            "Iteration   0 | Train Loss  0.2780 | Classifier Accuracy 89.06\n",
            "Iteration  20 | Train Loss  0.2262 | Classifier Accuracy 92.19\n",
            "Iteration  40 | Train Loss  0.4113 | Classifier Accuracy 81.25\n",
            "Iteration  60 | Train Loss  0.2307 | Classifier Accuracy 93.75\n",
            "Iteration  80 | Train Loss  0.2848 | Classifier Accuracy 87.50\n",
            "Iteration 100 | Train Loss  0.3388 | Classifier Accuracy 89.06\n",
            "Iteration 120 | Train Loss  0.2570 | Classifier Accuracy 92.19\n",
            "Iteration 140 | Train Loss  0.4743 | Classifier Accuracy 79.69\n",
            "Iteration 160 | Train Loss  0.5051 | Classifier Accuracy 73.44\n",
            "Iteration 180 | Train Loss  0.3300 | Classifier Accuracy 87.50\n",
            "Iteration 200 | Train Loss  0.4767 | Classifier Accuracy 78.12\n",
            "Iteration 220 | Train Loss  0.4517 | Classifier Accuracy 76.56\n",
            "Iteration 240 | Train Loss  0.2020 | Classifier Accuracy 92.19\n",
            "Iteration 260 | Train Loss  0.2528 | Classifier Accuracy 89.06\n",
            "Iteration 280 | Train Loss  0.2864 | Classifier Accuracy 87.50\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.3512 | Accuracy 84.50 \n",
            "Valid Loss Mean 0.3793 | Accuracy 81.69 \n",
            "\n",
            " ====================== epoch 5 ======================\n",
            "Iteration   0 | Train Loss  0.3589 | Classifier Accuracy 87.50\n",
            "Iteration  20 | Train Loss  0.3625 | Classifier Accuracy 85.94\n",
            "Iteration  40 | Train Loss  0.1682 | Classifier Accuracy 92.19\n",
            "Iteration  60 | Train Loss  0.2460 | Classifier Accuracy 89.06\n",
            "Iteration  80 | Train Loss  0.1960 | Classifier Accuracy 95.31\n",
            "Iteration 100 | Train Loss  0.2498 | Classifier Accuracy 85.94\n",
            "Iteration 120 | Train Loss  0.1989 | Classifier Accuracy 92.19\n",
            "Iteration 140 | Train Loss  0.2918 | Classifier Accuracy 87.50\n",
            "Iteration 160 | Train Loss  0.2220 | Classifier Accuracy 89.06\n",
            "Iteration 180 | Train Loss  0.3111 | Classifier Accuracy 87.50\n",
            "Iteration 200 | Train Loss  0.2336 | Classifier Accuracy 92.19\n",
            "Iteration 220 | Train Loss  0.3630 | Classifier Accuracy 82.81\n",
            "Iteration 240 | Train Loss  0.2854 | Classifier Accuracy 89.06\n",
            "Iteration 260 | Train Loss  0.3160 | Classifier Accuracy 84.38\n",
            "Iteration 280 | Train Loss  0.3332 | Classifier Accuracy 87.50\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.2742 | Accuracy 88.42 \n",
            "Valid Loss Mean 0.4190 | Accuracy 81.05 \n",
            "\n",
            " ====================== epoch 6 ======================\n",
            "Iteration   0 | Train Loss  0.2377 | Classifier Accuracy 93.75\n",
            "Iteration  20 | Train Loss  0.2917 | Classifier Accuracy 89.06\n",
            "Iteration  40 | Train Loss  0.1761 | Classifier Accuracy 93.75\n",
            "Iteration  60 | Train Loss  0.0887 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.2689 | Classifier Accuracy 87.50\n",
            "Iteration 100 | Train Loss  0.1702 | Classifier Accuracy 93.75\n",
            "Iteration 120 | Train Loss  0.1462 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.0967 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.1355 | Classifier Accuracy 95.31\n",
            "Iteration 180 | Train Loss  0.2891 | Classifier Accuracy 89.06\n",
            "Iteration 200 | Train Loss  0.2026 | Classifier Accuracy 92.19\n",
            "Iteration 220 | Train Loss  0.2036 | Classifier Accuracy 95.31\n",
            "Iteration 240 | Train Loss  0.2074 | Classifier Accuracy 90.62\n",
            "Iteration 260 | Train Loss  0.1997 | Classifier Accuracy 92.19\n",
            "Iteration 280 | Train Loss  0.2590 | Classifier Accuracy 90.62\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.2045 | Accuracy 91.53 \n",
            "Valid Loss Mean 0.2795 | Accuracy 86.33 \n",
            "\n",
            " ====================== epoch 7 ======================\n",
            "Iteration   0 | Train Loss  0.1537 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.1025 | Classifier Accuracy 95.31\n",
            "Iteration  40 | Train Loss  0.0765 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.4029 | Classifier Accuracy 82.81\n",
            "Iteration  80 | Train Loss  0.0682 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.1087 | Classifier Accuracy 93.75\n",
            "Iteration 120 | Train Loss  0.2068 | Classifier Accuracy 87.50\n",
            "Iteration 140 | Train Loss  0.1875 | Classifier Accuracy 93.75\n",
            "Iteration 160 | Train Loss  0.0913 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.1261 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.2132 | Classifier Accuracy 90.62\n",
            "Iteration 220 | Train Loss  0.2414 | Classifier Accuracy 89.06\n",
            "Iteration 240 | Train Loss  0.1831 | Classifier Accuracy 92.19\n",
            "Iteration 260 | Train Loss  0.1059 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.2002 | Classifier Accuracy 90.62\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.1530 | Accuracy 94.04 \n",
            "Valid Loss Mean 0.2580 | Accuracy 86.57 \n",
            "\n",
            " ====================== epoch 8 ======================\n",
            "Iteration   0 | Train Loss  0.1050 | Classifier Accuracy 95.31\n",
            "Iteration  20 | Train Loss  0.1364 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.1250 | Classifier Accuracy 95.31\n",
            "Iteration  60 | Train Loss  0.1333 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.1226 | Classifier Accuracy 95.31\n",
            "Iteration 100 | Train Loss  0.0483 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.1604 | Classifier Accuracy 95.31\n",
            "Iteration 140 | Train Loss  0.0715 | Classifier Accuracy 96.88\n",
            "Iteration 160 | Train Loss  0.1063 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.1287 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.2683 | Classifier Accuracy 90.62\n",
            "Iteration 220 | Train Loss  0.0801 | Classifier Accuracy 95.31\n",
            "Iteration 240 | Train Loss  0.1551 | Classifier Accuracy 92.19\n",
            "Iteration 260 | Train Loss  0.1169 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.1947 | Classifier Accuracy 90.62\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.1242 | Accuracy 95.21 \n",
            "Valid Loss Mean 0.3194 | Accuracy 83.98 \n",
            "\n",
            " ====================== epoch 9 ======================\n",
            "Iteration   0 | Train Loss  0.1039 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.0442 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.1059 | Classifier Accuracy 92.19\n",
            "Iteration  60 | Train Loss  0.1289 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.0851 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.2749 | Classifier Accuracy 90.62\n",
            "Iteration 120 | Train Loss  0.0595 | Classifier Accuracy 96.88\n",
            "Iteration 140 | Train Loss  0.2710 | Classifier Accuracy 89.06\n",
            "Iteration 160 | Train Loss  0.1856 | Classifier Accuracy 92.19\n",
            "Iteration 180 | Train Loss  0.0491 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.1226 | Classifier Accuracy 92.19\n",
            "Iteration 220 | Train Loss  0.2491 | Classifier Accuracy 89.06\n",
            "Iteration 240 | Train Loss  0.0538 | Classifier Accuracy 96.88\n",
            "Iteration 260 | Train Loss  0.0934 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.1960 | Classifier Accuracy 95.31\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.1077 | Accuracy 95.58 \n",
            "Valid Loss Mean 0.2688 | Accuracy 88.23 \n",
            "\n",
            " ====================== epoch 10 ======================\n",
            "Iteration   0 | Train Loss  0.0813 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.1371 | Classifier Accuracy 95.31\n",
            "Iteration  40 | Train Loss  0.0347 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0396 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0411 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.0124 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.1215 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.1260 | Classifier Accuracy 95.31\n",
            "Iteration 160 | Train Loss  0.0272 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.1703 | Classifier Accuracy 93.75\n",
            "Iteration 200 | Train Loss  0.0300 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.1384 | Classifier Accuracy 96.88\n",
            "Iteration 240 | Train Loss  0.1015 | Classifier Accuracy 95.31\n",
            "Iteration 260 | Train Loss  0.0416 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.1024 | Classifier Accuracy 95.31\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0872 | Accuracy 96.53 \n",
            "Valid Loss Mean 0.2946 | Accuracy 88.96 \n",
            "\n",
            " ====================== epoch 11 ======================\n",
            "Iteration   0 | Train Loss  0.0322 | Classifier Accuracy 98.44\n",
            "Iteration  20 | Train Loss  0.0665 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0384 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.0148 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0039 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0797 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.0156 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0093 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0601 | Classifier Accuracy 98.44\n",
            "Iteration 180 | Train Loss  0.0466 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.1266 | Classifier Accuracy 95.31\n",
            "Iteration 220 | Train Loss  0.0931 | Classifier Accuracy 96.88\n",
            "Iteration 240 | Train Loss  0.0128 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.2217 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.0364 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0651 | Accuracy 97.31 \n",
            "Valid Loss Mean 0.2998 | Accuracy 88.92 \n",
            "\n",
            " ====================== epoch 12 ======================\n",
            "Iteration   0 | Train Loss  0.0185 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0390 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.1521 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.0676 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.0482 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.0198 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0274 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0303 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0101 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.2011 | Classifier Accuracy 93.75\n",
            "Iteration 200 | Train Loss  0.0562 | Classifier Accuracy 96.88\n",
            "Iteration 220 | Train Loss  0.0244 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0798 | Classifier Accuracy 95.31\n",
            "Iteration 260 | Train Loss  0.0720 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0790 | Classifier Accuracy 96.88\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0607 | Accuracy 97.70 \n",
            "Valid Loss Mean 0.3871 | Accuracy 87.11 \n",
            "\n",
            " ====================== epoch 13 ======================\n",
            "Iteration   0 | Train Loss  0.0125 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0900 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0332 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0271 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0680 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.0198 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.0135 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0509 | Classifier Accuracy 96.88\n",
            "Iteration 160 | Train Loss  0.0092 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0606 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.0731 | Classifier Accuracy 98.44\n",
            "Iteration 220 | Train Loss  0.0630 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0158 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0779 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0167 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0472 | Accuracy 98.07 \n",
            "Valid Loss Mean 0.3093 | Accuracy 89.70 \n",
            "\n",
            "EARLY STOPPING!!\n",
            "[fold: 3]\n",
            " ====================== epoch 1 ======================\n",
            "Iteration   0 | Train Loss  0.7042 | Classifier Accuracy 50.00\n",
            "Iteration  20 | Train Loss  0.6992 | Classifier Accuracy 64.06\n",
            "Iteration  40 | Train Loss  0.7926 | Classifier Accuracy 40.62\n",
            "Iteration  60 | Train Loss  0.6823 | Classifier Accuracy 57.81\n",
            "Iteration  80 | Train Loss  0.7438 | Classifier Accuracy 42.19\n",
            "Iteration 100 | Train Loss  0.7580 | Classifier Accuracy 45.31\n",
            "Iteration 120 | Train Loss  0.6848 | Classifier Accuracy 51.56\n",
            "Iteration 140 | Train Loss  0.7278 | Classifier Accuracy 46.88\n",
            "Iteration 160 | Train Loss  0.6710 | Classifier Accuracy 56.25\n",
            "Iteration 180 | Train Loss  0.8005 | Classifier Accuracy 60.94\n",
            "Iteration 200 | Train Loss  0.6628 | Classifier Accuracy 60.94\n",
            "Iteration 220 | Train Loss  0.6605 | Classifier Accuracy 56.25\n",
            "Iteration 240 | Train Loss  0.7000 | Classifier Accuracy 56.25\n",
            "Iteration 260 | Train Loss  0.6397 | Classifier Accuracy 57.81\n",
            "Iteration 280 | Train Loss  0.6007 | Classifier Accuracy 62.50\n",
            "\n",
            "[Summary] Elapsed time : 2 m 12 s\n",
            "Train Loss Mean 0.6958 | Accuracy 56.01 \n",
            "Valid Loss Mean 0.6965 | Accuracy 47.56 \n",
            "\n",
            " ====================== epoch 2 ======================\n",
            "Iteration   0 | Train Loss  0.6446 | Classifier Accuracy 68.75\n",
            "Iteration  20 | Train Loss  0.5728 | Classifier Accuracy 67.19\n",
            "Iteration  40 | Train Loss  0.5391 | Classifier Accuracy 71.88\n",
            "Iteration  60 | Train Loss  0.5136 | Classifier Accuracy 78.12\n",
            "Iteration  80 | Train Loss  0.5501 | Classifier Accuracy 67.19\n",
            "Iteration 100 | Train Loss  0.5093 | Classifier Accuracy 76.56\n",
            "Iteration 120 | Train Loss  0.6083 | Classifier Accuracy 73.44\n",
            "Iteration 140 | Train Loss  0.5243 | Classifier Accuracy 71.88\n",
            "Iteration 160 | Train Loss  0.6842 | Classifier Accuracy 64.06\n",
            "Iteration 180 | Train Loss  0.6069 | Classifier Accuracy 68.75\n",
            "Iteration 200 | Train Loss  0.4901 | Classifier Accuracy 79.69\n",
            "Iteration 220 | Train Loss  0.7255 | Classifier Accuracy 65.62\n",
            "Iteration 240 | Train Loss  0.5180 | Classifier Accuracy 78.12\n",
            "Iteration 260 | Train Loss  0.4705 | Classifier Accuracy 78.12\n",
            "Iteration 280 | Train Loss  0.4719 | Classifier Accuracy 82.81\n",
            "\n",
            "[Summary] Elapsed time : 2 m 24 s\n",
            "Train Loss Mean 0.5592 | Accuracy 71.64 \n",
            "Valid Loss Mean 0.5305 | Accuracy 72.22 \n",
            "\n",
            " ====================== epoch 3 ======================\n",
            "Iteration   0 | Train Loss  0.4051 | Classifier Accuracy 84.38\n",
            "Iteration  20 | Train Loss  0.4902 | Classifier Accuracy 81.25\n",
            "Iteration  40 | Train Loss  0.3871 | Classifier Accuracy 84.38\n",
            "Iteration  60 | Train Loss  0.4848 | Classifier Accuracy 78.12\n",
            "Iteration  80 | Train Loss  0.3546 | Classifier Accuracy 85.94\n",
            "Iteration 100 | Train Loss  0.3286 | Classifier Accuracy 89.06\n",
            "Iteration 120 | Train Loss  0.3525 | Classifier Accuracy 79.69\n",
            "Iteration 140 | Train Loss  0.4426 | Classifier Accuracy 76.56\n",
            "Iteration 160 | Train Loss  0.5168 | Classifier Accuracy 75.00\n",
            "Iteration 180 | Train Loss  0.3705 | Classifier Accuracy 85.94\n",
            "Iteration 200 | Train Loss  0.4369 | Classifier Accuracy 82.81\n",
            "Iteration 220 | Train Loss  0.3459 | Classifier Accuracy 84.38\n",
            "Iteration 240 | Train Loss  0.4123 | Classifier Accuracy 78.12\n",
            "Iteration 260 | Train Loss  0.3675 | Classifier Accuracy 84.38\n",
            "Iteration 280 | Train Loss  0.3398 | Classifier Accuracy 85.94\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.4143 | Accuracy 81.08 \n",
            "Valid Loss Mean 2.4584 | Accuracy 71.44 \n",
            "\n",
            " ====================== epoch 4 ======================\n",
            "Iteration   0 | Train Loss  0.3368 | Classifier Accuracy 84.38\n",
            "Iteration  20 | Train Loss  0.3829 | Classifier Accuracy 82.81\n",
            "Iteration  40 | Train Loss  0.3953 | Classifier Accuracy 84.38\n",
            "Iteration  60 | Train Loss  0.3445 | Classifier Accuracy 84.38\n",
            "Iteration  80 | Train Loss  0.4135 | Classifier Accuracy 85.94\n",
            "Iteration 100 | Train Loss  0.3305 | Classifier Accuracy 84.38\n",
            "Iteration 120 | Train Loss  0.3988 | Classifier Accuracy 82.81\n",
            "Iteration 140 | Train Loss  0.5468 | Classifier Accuracy 73.44\n",
            "Iteration 160 | Train Loss  0.3707 | Classifier Accuracy 81.25\n",
            "Iteration 180 | Train Loss  0.3041 | Classifier Accuracy 89.06\n",
            "Iteration 200 | Train Loss  0.3513 | Classifier Accuracy 87.50\n",
            "Iteration 220 | Train Loss  0.2017 | Classifier Accuracy 93.75\n",
            "Iteration 240 | Train Loss  0.3877 | Classifier Accuracy 84.38\n",
            "Iteration 260 | Train Loss  0.1519 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.4387 | Classifier Accuracy 79.69\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.3532 | Accuracy 84.38 \n",
            "Valid Loss Mean 0.3903 | Accuracy 79.93 \n",
            "\n",
            " ====================== epoch 5 ======================\n",
            "Iteration   0 | Train Loss  0.3819 | Classifier Accuracy 82.81\n",
            "Iteration  20 | Train Loss  0.2579 | Classifier Accuracy 90.62\n",
            "Iteration  40 | Train Loss  0.1554 | Classifier Accuracy 92.19\n",
            "Iteration  60 | Train Loss  0.2826 | Classifier Accuracy 82.81\n",
            "Iteration  80 | Train Loss  0.2594 | Classifier Accuracy 87.50\n",
            "Iteration 100 | Train Loss  0.3610 | Classifier Accuracy 82.81\n",
            "Iteration 120 | Train Loss  0.5050 | Classifier Accuracy 76.56\n",
            "Iteration 140 | Train Loss  0.2600 | Classifier Accuracy 89.06\n",
            "Iteration 160 | Train Loss  0.1367 | Classifier Accuracy 95.31\n",
            "Iteration 180 | Train Loss  0.4458 | Classifier Accuracy 75.00\n",
            "Iteration 200 | Train Loss  0.2204 | Classifier Accuracy 93.75\n",
            "Iteration 220 | Train Loss  0.1636 | Classifier Accuracy 93.75\n",
            "Iteration 240 | Train Loss  0.2360 | Classifier Accuracy 90.62\n",
            "Iteration 260 | Train Loss  0.2602 | Classifier Accuracy 89.06\n",
            "Iteration 280 | Train Loss  0.1859 | Classifier Accuracy 93.75\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.2634 | Accuracy 88.75 \n",
            "Valid Loss Mean 0.3554 | Accuracy 82.57 \n",
            "\n",
            " ====================== epoch 6 ======================\n",
            "Iteration   0 | Train Loss  0.2525 | Classifier Accuracy 87.50\n",
            "Iteration  20 | Train Loss  0.2342 | Classifier Accuracy 89.06\n",
            "Iteration  40 | Train Loss  0.1486 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.3025 | Classifier Accuracy 90.62\n",
            "Iteration  80 | Train Loss  0.2087 | Classifier Accuracy 87.50\n",
            "Iteration 100 | Train Loss  0.1280 | Classifier Accuracy 92.19\n",
            "Iteration 120 | Train Loss  0.1680 | Classifier Accuracy 95.31\n",
            "Iteration 140 | Train Loss  0.1969 | Classifier Accuracy 95.31\n",
            "Iteration 160 | Train Loss  0.2467 | Classifier Accuracy 87.50\n",
            "Iteration 180 | Train Loss  0.1930 | Classifier Accuracy 93.75\n",
            "Iteration 200 | Train Loss  0.2243 | Classifier Accuracy 90.62\n",
            "Iteration 220 | Train Loss  0.2505 | Classifier Accuracy 90.62\n",
            "Iteration 240 | Train Loss  0.1694 | Classifier Accuracy 89.06\n",
            "Iteration 260 | Train Loss  0.3084 | Classifier Accuracy 89.06\n",
            "Iteration 280 | Train Loss  0.4528 | Classifier Accuracy 84.38\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.2201 | Accuracy 90.79 \n",
            "Valid Loss Mean 0.2960 | Accuracy 85.69 \n",
            "\n",
            " ====================== epoch 7 ======================\n",
            "Iteration   0 | Train Loss  0.1471 | Classifier Accuracy 92.19\n",
            "Iteration  20 | Train Loss  0.2022 | Classifier Accuracy 90.62\n",
            "Iteration  40 | Train Loss  0.2574 | Classifier Accuracy 87.50\n",
            "Iteration  60 | Train Loss  0.1307 | Classifier Accuracy 95.31\n",
            "Iteration  80 | Train Loss  0.1689 | Classifier Accuracy 93.75\n",
            "Iteration 100 | Train Loss  0.0995 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.2224 | Classifier Accuracy 95.31\n",
            "Iteration 140 | Train Loss  0.1107 | Classifier Accuracy 93.75\n",
            "Iteration 160 | Train Loss  0.2548 | Classifier Accuracy 87.50\n",
            "Iteration 180 | Train Loss  0.2352 | Classifier Accuracy 87.50\n",
            "Iteration 200 | Train Loss  0.3618 | Classifier Accuracy 87.50\n",
            "Iteration 220 | Train Loss  0.1411 | Classifier Accuracy 92.19\n",
            "Iteration 240 | Train Loss  0.1751 | Classifier Accuracy 90.62\n",
            "Iteration 260 | Train Loss  0.2229 | Classifier Accuracy 89.06\n",
            "Iteration 280 | Train Loss  0.4680 | Classifier Accuracy 78.12\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.2034 | Accuracy 91.34 \n",
            "Valid Loss Mean 47.3381 | Accuracy 47.56 \n",
            "\n",
            " ====================== epoch 8 ======================\n",
            "Iteration   0 | Train Loss  0.4706 | Classifier Accuracy 75.00\n",
            "Iteration  20 | Train Loss  0.6693 | Classifier Accuracy 64.06\n",
            "Iteration  40 | Train Loss  0.4026 | Classifier Accuracy 84.38\n",
            "Iteration  60 | Train Loss  0.3650 | Classifier Accuracy 82.81\n",
            "Iteration  80 | Train Loss  0.3508 | Classifier Accuracy 82.81\n",
            "Iteration 100 | Train Loss  0.2275 | Classifier Accuracy 87.50\n",
            "Iteration 120 | Train Loss  0.2758 | Classifier Accuracy 89.06\n",
            "Iteration 140 | Train Loss  0.2829 | Classifier Accuracy 90.62\n",
            "Iteration 160 | Train Loss  0.1494 | Classifier Accuracy 93.75\n",
            "Iteration 180 | Train Loss  0.2258 | Classifier Accuracy 90.62\n",
            "Iteration 200 | Train Loss  0.2217 | Classifier Accuracy 89.06\n",
            "Iteration 220 | Train Loss  0.1526 | Classifier Accuracy 92.19\n",
            "Iteration 240 | Train Loss  0.3437 | Classifier Accuracy 84.38\n",
            "Iteration 260 | Train Loss  0.1717 | Classifier Accuracy 92.19\n",
            "Iteration 280 | Train Loss  0.2406 | Classifier Accuracy 93.75\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.3007 | Accuracy 87.09 \n",
            "Valid Loss Mean 0.2902 | Accuracy 86.43 \n",
            "\n",
            " ====================== epoch 9 ======================\n",
            "Iteration   0 | Train Loss  0.1278 | Classifier Accuracy 95.31\n",
            "Iteration  20 | Train Loss  0.0841 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.0983 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.2650 | Classifier Accuracy 92.19\n",
            "Iteration  80 | Train Loss  0.1494 | Classifier Accuracy 93.75\n",
            "Iteration 100 | Train Loss  0.1613 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.1492 | Classifier Accuracy 93.75\n",
            "Iteration 140 | Train Loss  0.1795 | Classifier Accuracy 92.19\n",
            "Iteration 160 | Train Loss  0.0981 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0594 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.2396 | Classifier Accuracy 89.06\n",
            "Iteration 220 | Train Loss  0.1622 | Classifier Accuracy 95.31\n",
            "Iteration 240 | Train Loss  0.1628 | Classifier Accuracy 90.62\n",
            "Iteration 260 | Train Loss  0.0894 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.3198 | Classifier Accuracy 85.94\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.1475 | Accuracy 93.93 \n",
            "Valid Loss Mean 0.2989 | Accuracy 86.62 \n",
            "\n",
            " ====================== epoch 10 ======================\n",
            "Iteration   0 | Train Loss  0.1092 | Classifier Accuracy 95.31\n",
            "Iteration  20 | Train Loss  0.0644 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0800 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.1480 | Classifier Accuracy 92.19\n",
            "Iteration  80 | Train Loss  0.0666 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0734 | Classifier Accuracy 96.88\n",
            "Iteration 120 | Train Loss  0.0504 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0681 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.2902 | Classifier Accuracy 93.75\n",
            "Iteration 180 | Train Loss  0.0372 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.1594 | Classifier Accuracy 93.75\n",
            "Iteration 220 | Train Loss  0.0201 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.2006 | Classifier Accuracy 92.19\n",
            "Iteration 260 | Train Loss  0.0798 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.1341 | Classifier Accuracy 93.75\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.1093 | Accuracy 95.64 \n",
            "Valid Loss Mean 0.3577 | Accuracy 86.33 \n",
            "\n",
            " ====================== epoch 11 ======================\n",
            "Iteration   0 | Train Loss  0.1785 | Classifier Accuracy 95.31\n",
            "Iteration  20 | Train Loss  0.0321 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.1128 | Classifier Accuracy 95.31\n",
            "Iteration  60 | Train Loss  0.0211 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0149 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.1015 | Classifier Accuracy 95.31\n",
            "Iteration 120 | Train Loss  0.0397 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0775 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.1190 | Classifier Accuracy 90.62\n",
            "Iteration 180 | Train Loss  0.0615 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.0756 | Classifier Accuracy 96.88\n",
            "Iteration 220 | Train Loss  0.0258 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0508 | Classifier Accuracy 96.88\n",
            "Iteration 260 | Train Loss  0.0774 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0687 | Classifier Accuracy 95.31\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0812 | Accuracy 96.81 \n",
            "Valid Loss Mean 0.3117 | Accuracy 87.55 \n",
            "\n",
            " ====================== epoch 12 ======================\n",
            "Iteration   0 | Train Loss  0.1369 | Classifier Accuracy 93.75\n",
            "Iteration  20 | Train Loss  0.0812 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0739 | Classifier Accuracy 95.31\n",
            "Iteration  60 | Train Loss  0.0802 | Classifier Accuracy 93.75\n",
            "Iteration  80 | Train Loss  0.0552 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0160 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0472 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0339 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.1018 | Classifier Accuracy 93.75\n",
            "Iteration 180 | Train Loss  0.0772 | Classifier Accuracy 95.31\n",
            "Iteration 200 | Train Loss  0.1029 | Classifier Accuracy 95.31\n",
            "Iteration 220 | Train Loss  0.0612 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.1425 | Classifier Accuracy 93.75\n",
            "Iteration 260 | Train Loss  0.1098 | Classifier Accuracy 96.88\n",
            "Iteration 280 | Train Loss  0.0710 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0587 | Accuracy 97.61 \n",
            "Valid Loss Mean 0.2907 | Accuracy 88.92 \n",
            "\n",
            " ====================== epoch 13 ======================\n",
            "Iteration   0 | Train Loss  0.0164 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0336 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.0348 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0355 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0279 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0461 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0264 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0530 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0476 | Classifier Accuracy 96.88\n",
            "Iteration 200 | Train Loss  0.0304 | Classifier Accuracy 98.44\n",
            "Iteration 220 | Train Loss  0.0394 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0112 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0082 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0259 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0501 | Accuracy 97.82 \n",
            "Valid Loss Mean 0.2838 | Accuracy 88.53 \n",
            "\n",
            " ====================== epoch 14 ======================\n",
            "Iteration   0 | Train Loss  0.0338 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0549 | Classifier Accuracy 96.88\n",
            "Iteration  40 | Train Loss  0.0227 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0424 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.1549 | Classifier Accuracy 96.88\n",
            "Iteration 100 | Train Loss  0.0114 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0211 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0368 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.0222 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0045 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0031 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0965 | Classifier Accuracy 95.31\n",
            "Iteration 260 | Train Loss  0.0148 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0118 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 21 s\n",
            "Train Loss Mean 0.0396 | Accuracy 98.30 \n",
            "Valid Loss Mean 0.3286 | Accuracy 88.72 \n",
            "\n",
            " ====================== epoch 15 ======================\n",
            "Iteration   0 | Train Loss  0.0090 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0082 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0599 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.1125 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.0358 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0341 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.1032 | Classifier Accuracy 96.88\n",
            "Iteration 140 | Train Loss  0.0564 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.1037 | Classifier Accuracy 96.88\n",
            "Iteration 180 | Train Loss  0.1702 | Classifier Accuracy 93.75\n",
            "Iteration 200 | Train Loss  0.0115 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0628 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0108 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0143 | Classifier Accuracy 100.00\n",
            "Iteration 280 | Train Loss  0.0229 | Classifier Accuracy 100.00\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0367 | Accuracy 98.59 \n",
            "Valid Loss Mean 0.3696 | Accuracy 88.67 \n",
            "\n",
            " ====================== epoch 16 ======================\n",
            "Iteration   0 | Train Loss  0.1096 | Classifier Accuracy 96.88\n",
            "Iteration  20 | Train Loss  0.1037 | Classifier Accuracy 95.31\n",
            "Iteration  40 | Train Loss  0.0709 | Classifier Accuracy 95.31\n",
            "Iteration  60 | Train Loss  0.0307 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.1094 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0440 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.0702 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0101 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0075 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0681 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.0027 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0140 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0402 | Classifier Accuracy 96.88\n",
            "Iteration 260 | Train Loss  0.0355 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0551 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0395 | Accuracy 98.28 \n",
            "Valid Loss Mean 0.3278 | Accuracy 88.62 \n",
            "\n",
            " ====================== epoch 17 ======================\n",
            "Iteration   0 | Train Loss  0.0051 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0036 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0155 | Classifier Accuracy 98.44\n",
            "Iteration  60 | Train Loss  0.0210 | Classifier Accuracy 98.44\n",
            "Iteration  80 | Train Loss  0.0190 | Classifier Accuracy 98.44\n",
            "Iteration 100 | Train Loss  0.0198 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0432 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0670 | Classifier Accuracy 98.44\n",
            "Iteration 180 | Train Loss  0.0053 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0414 | Classifier Accuracy 98.44\n",
            "Iteration 220 | Train Loss  0.0521 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0072 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0536 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0477 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0208 | Accuracy 99.03 \n",
            "Valid Loss Mean 0.4137 | Accuracy 86.38 \n",
            "\n",
            " ====================== epoch 18 ======================\n",
            "Iteration   0 | Train Loss  0.0381 | Classifier Accuracy 96.88\n",
            "Iteration  20 | Train Loss  0.0435 | Classifier Accuracy 98.44\n",
            "Iteration  40 | Train Loss  0.0550 | Classifier Accuracy 96.88\n",
            "Iteration  60 | Train Loss  0.0154 | Classifier Accuracy 100.00\n",
            "Iteration  80 | Train Loss  0.0056 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0009 | Classifier Accuracy 100.00\n",
            "Iteration 120 | Train Loss  0.0340 | Classifier Accuracy 98.44\n",
            "Iteration 140 | Train Loss  0.0405 | Classifier Accuracy 98.44\n",
            "Iteration 160 | Train Loss  0.0127 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0352 | Classifier Accuracy 98.44\n",
            "Iteration 200 | Train Loss  0.0019 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0225 | Classifier Accuracy 98.44\n",
            "Iteration 240 | Train Loss  0.0342 | Classifier Accuracy 98.44\n",
            "Iteration 260 | Train Loss  0.0176 | Classifier Accuracy 98.44\n",
            "Iteration 280 | Train Loss  0.0211 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 19 s\n",
            "Train Loss Mean 0.0189 | Accuracy 99.06 \n",
            "Valid Loss Mean 0.2973 | Accuracy 89.79 \n",
            "\n",
            " ====================== epoch 19 ======================\n",
            "Iteration   0 | Train Loss  0.0008 | Classifier Accuracy 100.00\n",
            "Iteration  20 | Train Loss  0.0138 | Classifier Accuracy 100.00\n",
            "Iteration  40 | Train Loss  0.0065 | Classifier Accuracy 100.00\n",
            "Iteration  60 | Train Loss  0.0472 | Classifier Accuracy 96.88\n",
            "Iteration  80 | Train Loss  0.0070 | Classifier Accuracy 100.00\n",
            "Iteration 100 | Train Loss  0.0512 | Classifier Accuracy 98.44\n",
            "Iteration 120 | Train Loss  0.0011 | Classifier Accuracy 100.00\n",
            "Iteration 140 | Train Loss  0.0034 | Classifier Accuracy 100.00\n",
            "Iteration 160 | Train Loss  0.0017 | Classifier Accuracy 100.00\n",
            "Iteration 180 | Train Loss  0.0020 | Classifier Accuracy 100.00\n",
            "Iteration 200 | Train Loss  0.0043 | Classifier Accuracy 100.00\n",
            "Iteration 220 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
            "Iteration 240 | Train Loss  0.0012 | Classifier Accuracy 100.00\n",
            "Iteration 260 | Train Loss  0.0835 | Classifier Accuracy 95.31\n",
            "Iteration 280 | Train Loss  0.0163 | Classifier Accuracy 98.44\n",
            "\n",
            "[Summary] Elapsed time : 2 m 20 s\n",
            "Train Loss Mean 0.0168 | Accuracy 99.05 \n",
            "Valid Loss Mean 0.3476 | Accuracy 91.26 \n",
            "\n",
            "EARLY STOPPING!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APypQ303FxjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "26e2fea9-746b-404a-cc9d-a48c11f41d60"
      },
      "source": [
        "submission = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/통기계플젝/face_image/submission.csv\")\n",
        "submission.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test14200.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test12178.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test12713.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test13712.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test11739.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           image  label\n",
              "0  test14200.jpg    NaN\n",
              "1  test12178.jpg    NaN\n",
              "2  test12713.jpg    NaN\n",
              "3  test13712.jpg    NaN\n",
              "4  test11739.jpg    NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_UWT6Yv0VzS"
      },
      "source": [
        "import numpy as np\n",
        "# softmax_0: 0으로 분류할 확률\n",
        "def softmax_0(score0, score1) :\n",
        "  exp_a0 = np.exp(score0)\n",
        "  exp_a1 = np.exp(score1)\n",
        "  sum_exp = exp_a0+exp_a1\n",
        "  y = exp_a0 / sum_exp\n",
        "  return y\n",
        "\n",
        "# ensemble 시 threshold 설정을 위해 모든 score를 probability로 변환"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcSflooQvQMs"
      },
      "source": [
        "# Efficient Net inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkm-Gr3r4_dN"
      },
      "source": [
        "# inference에 사용할 model과 optimizer\n",
        "model = efficientnet()\n",
        "optimizer = optim.RAdam(model.parameters(), lr=0.0015, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "\n",
        "prob_zero = [0 for _ in range(5000)]\n",
        "\n",
        "for i in range(1, 4):\n",
        "  checkpoint = torch.load(f'/content/drive/MyDrive/Colab Notebooks/통기계플젝/model/efficient{i}-fold_best-model.pth')  # 불러올 모델의 이름에서 fold 번호를 바꿔가면서 각 fold의 best model load \n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval() # model load 후, inference를 위한 eval 모드로 다시 설정\n",
        "\n",
        "  predictions = []\n",
        "  files = []\n",
        "  score_list = []\n",
        "  test_dataset = TestDataset(submission['image'], transforms_val)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for img_names, images in test_loader:\n",
        "      images = images.to(device=device, dtype=dtype)\n",
        "      scores = model(images)\n",
        "      _, preds = scores.max(dim=1)\n",
        "      \n",
        "      files.extend(img_names)\n",
        "      predictions.extend(preds.squeeze(0).detach().cpu().numpy())\n",
        "      score_list.extend(scores.squeeze(0).detach().cpu().numpy()) ## score 형태 그대로 리스트에 저장/ 형태 [[3.XXXXXX, -3.XXXXXX], [..], ...]\n",
        "  \n",
        "  # 확률 평균 구하기 위해 매 fold 별로 sum 저장\n",
        "  for i, score in enumerate(score_list):\n",
        "    prob_zero[i] += softmax_0(score[0], score[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRxWZ7cwwAne"
      },
      "source": [
        "# Xception Net train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO-PWcQs47Wo"
      },
      "source": [
        "num_epochs = 30\n",
        "EARLY_STOPPING_EPOCH = 6\n",
        "SEED = 42\n",
        "\n",
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
        "\n",
        "# train_df에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(train_df),1): # enumerate( , 1) 1부터 index 시작\n",
        "  print('[fold: %d]' % fold_index)\n",
        "\n",
        "  # cuda cache 초기화\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  #train fold, validation fold 분할\n",
        "  train = train_df.iloc[trn_idx]\n",
        "  valid  = train_df.iloc[val_idx]\n",
        "\n",
        "  tr_dataset = FaceDataset(image_label=train, transforms=transforms_tr)\n",
        "  val_dataset = FaceDataset(image_label=valid, transforms=transforms_val)\n",
        "\n",
        "  train_loader = DataLoader(tr_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
        "  valid_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
        "\n",
        "  # 모델 선언\n",
        "  model = xceptionnet()\n",
        "  model.to(device)\n",
        "\n",
        "  # optimizer : RAdam / lr_scheduler : CosineAnnealing\n",
        "  optimizer = optim.RAdam(model.parameters(), lr=0.0015, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "  lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "\n",
        "  # train 시작\n",
        "  valid_early_stop = 0\n",
        "\n",
        "  valid_best_loss = float('inf')\n",
        "  since = time.time()\n",
        "\n",
        "  final_train_loss = []\n",
        "  final_train_acc = []\n",
        "  final_valid_loss = []\n",
        "  final_valid_acc = []\n",
        "\n",
        "  for e in range(num_epochs) :\n",
        "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
        "    train_loss_list = []\n",
        "    train_acc_list = []\n",
        "\n",
        "    # train\n",
        "    for i, (images, targets) in enumerate(train_loader) : \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      images = images.to(device, dtype)\n",
        "      targets = targets.to(device, ltype)\n",
        "      model.train()\n",
        "    \n",
        "      scores = model(images)\n",
        "      _, preds = scores.max(dim=1)\n",
        "\n",
        "      loss = F.cross_entropy(scores, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      correct = sum(targets == preds).cpu()\n",
        "      acc=(correct/64 * 100)\n",
        "\n",
        "      train_loss_list.append(loss)\n",
        "      train_acc_list.append(acc)\n",
        "\n",
        "      if i % 20 == 0 :\n",
        "        print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
        "\n",
        "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
        "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
        "\n",
        "    final_train_loss.append(train_mean_loss)\n",
        "    final_train_acc.append(train_mean_acc)\n",
        "    \n",
        "    epoch_time = time.time() - since\n",
        "    since = time.time()\n",
        "\n",
        "    print('')\n",
        "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
        "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
        "\n",
        "    # validation \n",
        "    valid_loss_list = []\n",
        "    valid_acc_list = []\n",
        "   \n",
        "    for i, (images, targets) in enumerate(valid_loader) : \n",
        "      optimizer.zero_grad()\n",
        "      images = images.to(device=device, dtype=dtype)\n",
        "      targets = targets.to(device=device, dtype=ltype)\n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        scores = model(images)\n",
        "        loss = F.cross_entropy(scores, targets)\n",
        "        _, preds = scores.max(dim=1)\n",
        "      \n",
        "      correct = sum(targets == preds).cpu()\n",
        "      acc=(correct/64 * 100)\n",
        "\n",
        "      valid_loss_list.append(loss)\n",
        "      valid_acc_list.append(acc)\n",
        "  \n",
        "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
        "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
        "\n",
        "    final_valid_loss.append(val_mean_loss)\n",
        "    final_valid_acc.append(val_mean_acc)\n",
        "\n",
        "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
        "    print('')\n",
        "\n",
        "    if val_mean_loss < valid_best_loss:\n",
        "      valid_best_loss = val_mean_loss\n",
        "      valid_early_stop = 0\n",
        "      # new best model save (valid 기준)\n",
        "      best_model = model\n",
        "      path = '/content/drive/MyDrive/Colab Notebooks/통기계플젝/model/xception/'\n",
        "      torch.save({'model_state_dict' : best_model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict()}, f'{path}{fold_index}-fold_best-model.pth')\n",
        "\n",
        "    else:\n",
        "      # early stopping    \n",
        "      valid_early_stop += 1\n",
        "      if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
        "        print(\"EARLY STOPPING!!\")\n",
        "        break\n",
        "\n",
        "    lr_sched.step()\n",
        "  best_models.append(best_model)\n",
        "  if fold_index == 3:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VkwOx3LwLnG"
      },
      "source": [
        "# Xception Net inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B5RCmq5nqjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd3a13d-2b3e-4eda-b4d8-2c7104967fb6"
      },
      "source": [
        "# inference에 사용할 model과 optimizer\n",
        "#model = xceptionnet()\n",
        "model = timm.create_model('xception', pretrained=False)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(2048, 2)\n",
        ")\n",
        "optimizer = optim.RAdam(model.parameters(), lr=0.0015, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "\n",
        "for i in range(1, 4):\n",
        "  checkpoint = torch.load(f'/content/drive/MyDrive/Colab Notebooks/통기계플젝/model/xception/{i}-fold_best-model.pth')  # 불러올 모델의 이름에서 fold 번호를 바꿔가면서 각 fold의 best model load \n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval() # model load 후, inference를 위한 eval 모드로 다시 설정\n",
        "\n",
        "  predictions = []\n",
        "  files = []\n",
        "  score_list = []\n",
        "  test_dataset = TestDataset(submission['image'], transforms_val)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for img_names, images in test_loader:\n",
        "      images = images.to(device=device, dtype=dtype)\n",
        "      scores = model(images)\n",
        "      _, preds = scores.max(dim=1)\n",
        "      \n",
        "      files.extend(img_names)\n",
        "      predictions.extend(preds.squeeze(0).detach().cpu().numpy())\n",
        "      score_list.extend(scores.squeeze(0).detach().cpu().numpy()) ## score 형태 그대로 리스트에 저장/ 형태 [[3.XXXXXX, -3.XXXXXX], [..], ...]\n",
        "  \n",
        "  # 확률 평균 구하기 위해 매 fold 별로 sum 저장\n",
        "  for i, score in enumerate(score_list):\n",
        "    prob_zero[i] += softmax_0(score[0], score[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKNFzfnmwVAa"
      },
      "source": [
        "# Soft voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGxdCfp2kt9n"
      },
      "source": [
        "# 평균과 예측값 저장\n",
        "for i in range(5000):\n",
        "  prob_zero[i] /= 6 # fold 개수로 나누어 확률 평균 계산\n",
        "  prob_zero[i] = (prob_zero[i]<0.5)*1 # 0.5를 기준으로 예측값을 0과 1로 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ejX3ybown2"
      },
      "source": [
        "ensemble_sub = pd.DataFrame(columns=submission.columns)\n",
        "ensemble_sub['image'] = files\n",
        "ensemble_sub['label'] = prob_zero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAN4HbQx6iYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "44ebf133-0a1b-4bb9-a014-83992560e85c"
      },
      "source": [
        "ensemble_sub.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test14200.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test12178.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test12713.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test13712.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test11739.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           image  label\n",
              "0  test14200.jpg      0\n",
              "1  test12178.jpg      0\n",
              "2  test12713.jpg      0\n",
              "3  test13712.jpg      0\n",
              "4  test11739.jpg      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcWegxgu6wPY"
      },
      "source": [
        "ensemble_sub.to_csv(\"/content/drive/MyDrive/Colab Notebooks/통기계플젝/0601_EffNet4+XcpNet_soft.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}